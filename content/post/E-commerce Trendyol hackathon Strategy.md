---
title: "E commerce Trendyol hackathon Strategy"
date: 2026-02-16
type: post
tags: []
draft: false
---



## ðŸŽ¯ Competition Overview & Objectives

**Main Tasks:**

- **Click Prediction**: Predict which products users will click in a session
- **Purchase Prediction**: Predict which products users will buy in the same session
- **Evaluation**: Weighted sum of Recall@K scores for both tasks per session
- **Key Constraint**: No GPU/high-compute resources provided

**Success Metrics:**

- Primary: Recall@K (typically K=5 or K=20)
- Joint evaluation of click + purchase predictions
- Private leaderboard determines final ranking

---
https://github.com/otto-de/recsys-dataset
https://www.kaggle.com/datasets/otto/recsys-dataset
https://github.com/ahmadluay9/Ecommerce-Product-Recommendation-ChatGPT?tab=readme-ov-file
https://datawhalechina.github.io/fun-rec/#/ch01/ch1.3
https://session-based-recommenders.fastforwardlabs.com/#:~:text=There%20are%20many%20baselines%20for,of%20the%20user%E2%80%99s%20session%20history
https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54
https://medium.com/nvidia-merlin/winning-the-sigir-ecommerce-challenge-on-session-based-recommendation-with-transformers-v2-793f6fac2994
https://github.com/otto-de/recsys-dataset
https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54
https://arxiv.org/pdf/2507.19283
https://arxiv.org/pdf/2507.09566
https://arxiv.org/pdf/2409.12972
https://github.com/norahsakal/aws-titan-multimodal-embeddings-product-recommendation-system?tab=readme-ov-file
https://www.kaggle.com/code/fuzzywizard/rec-sys-collaborative-filtering-dl-techniques

Here are some highâ€‘quality code and walkthrough resources that closely align with your eâ€‘commerce sessionâ€‘based prediction goals (clicks & purchases with Recall@K evaluation):

---

## ðŸ“˜ Recommended Medium & Blog Articles

### 1. **Explaining OTTO Multiâ€‘Objective RecSys Architecture**

Blog posts based on the OTTO Kaggle competition dive into how they structure candidate generation (via Word2Vec and coâ€‘occurrence statistics), followed by ranking and Recall@20 submission format (clicks, carts, orders).

- These walkthrough blogs explain how to build Word2Vec embeddings from session sequences, generate candidate pools via â€œclickâ€‘toâ€‘clickâ€, â€œclickâ€‘toâ€‘orderâ€ transitions, cluster session embeddings, and then rank final topâ€‘20 lists for each prediction task ([GitHub](https://github.com/nicolaivicol/otto-recommender?utm_source=chatgpt.com "GitHub - nicolaivicol/otto-recommender: Solution to kaggle competition ...")).
    

### 2. **GRU4Rec & Sessionâ€‘Based RNNs Tutorial**

Mediumâ€‘style writeâ€‘ups (e.g. CSDN, personal blogs) summarize the original _Sessionâ€‘based Recommendations with RNNs_ (Hidasi et al., 2015), cover sessionâ€‘parallel minibatch training, rankingâ€‘loss (TOP1, BPR), negative sampling, and how GRU4Rec predicts next item in session ([CSDN Blog](https://blog.csdn.net/zhu_xian_gang/article/details/134722767?utm_source=chatgpt.com "GRU4Recå­¦ä¹ ç¬”è®°ï¼ˆSESSION-BASED RECOMMENDATIONS ... - CSDNåšå®¢")).

---

## ðŸ§° GitHub Repositories & Example Code

### 3. **Original GRU4Rec Implementation**

- The official Theano code release includes training scripts (`run.py`), evaluation (`evaluation.py`), sessionâ€‘parallel batching and negative sampling logic tailored to Recall@K style ranking tasks. It's the gold standard baseline ([GitHub](https://github.com/hidasib/GRU4Rec?utm_source=chatgpt.com "GitHub - hidasib/GRU4Rec: GRU4Rec is the original Theano implementation ...")).
    

### 4. **Keras & PyTorch Variants (with Caveats)**

- Community versions like `KerasGRU4Rec` offer easier APIs but often miss ranking loss or optimized negative samplingâ€”note performance gaps and limitations ([GitHub](https://github.com/paxcema/KerasGRU4Rec?utm_source=chatgpt.com "GitHub - paxcema/KerasGRU4Rec: Keras implementation of GRU4Rec session ...")).
    

### 5. **Sessionâ€‘Based Recommender Models Collection**

- PatrickSVMâ€™s GitHub includes implementations of GRU4Rec, as well as hybrid sequential models (SQN/SMORL) evaluated with Hitâ€‘Ratio, NDCG, etc. Great reference for code structure, batching and ranking evaluation ([GitHub](https://github.com/PatrickSVM/Session-Based-Recommender-Models?utm_source=chatgpt.com "Session Based Recommender Models - GitHub")).
    

### 6. **OTTO RecSys Dataset GitHub + Kaggle Guidelines**

- The official `otto-de/recsys-dataset` repo provides sample code for generating truncated evaluation sets, calculating Recall@20 per click/cart/order, and weighted scoring scheme used in the competition ([GitHub](https://github.com/otto-de/recsys-dataset/blob/main/KAGGLE.md?utm_source=chatgpt.com "recsys-dataset/KAGGLE.md at main Â· otto-de/recsys-dataset")).
    

---

## ðŸ—ï¸ Architecture Example Walkthrough

Hereâ€™s a simplified outline combining OTTO-style pipeline with GRU4Rec:

```python
# 1. Read sessions (JSONL): session_id, events = [(aid, ts, type), â€¦]
# 2. Build sequences: ordered aid lists grouped by session type
# 3. Generate candidates per session:
#    - Popular globally or by coâ€‘occurrence
#    - Word2Vec nearest neighbors
#    - Session cluster top items
# 4. Train ranking model:
#    - Option A: GRU4Rec RNN model to predict next aid given prior aids
#    - Option B: LightGBM ranker using engineered features (popularity, last aid, type counts)
# 5. Use ranking loss (e.g. TOP1, BPR) for GRU, crossâ€‘entropy or pointwise ranking for LightGBM
# 6. Evaluate offline with Recall@K per session/action, then compute weighted sum:  
#       score = 0.10â‹…Rc + 0.30â‹…Rcart + 0.60â‹…Rorder
```

This is essentially how participants constructed strong pipelines for OTTO: candidateâ€‘generation (Word2Vec & coâ€‘occurrence), followed by ranking/reranking and final Recall@K scoring ([duaibeom.github.io](https://duaibeom.github.io/blog/kaggle_otto?utm_source=chatgpt.com "RecSys Competition - OTTO")).

---

## ðŸ§  Further Academic Models (for deeper inspiration)

- **SRâ€‘GNN** (Session-based Recommendation with Graph Neural Networks): models sessions as graphs and uses GNN + attention to derive session embeddings and transitions. Stronger than classic RNNs in many benchmarks ([arXiv](https://arxiv.org/abs/1811.00855?utm_source=chatgpt.com "Session-based Recommendation with Graph Neural Networks")).
    
- **GRAINRec** (Novâ€¯2024): integrates Graph and Attention to enable realâ€‘time recommendations and dynamic updates midâ€‘session. A modern architecture that can inspire ensemble or lateâ€‘stage refinement ideas ([arXiv](https://arxiv.org/abs/2411.09152?utm_source=chatgpt.com "GRAINRec: Graph and Attention Integrated Approach for Real-Time Session-Based Item Recommendations")).
    
- **Hierarchical RNN** (HRNN): personalized RNN that includes crossâ€‘session user state to improve recommendations across sessions ([arXiv](https://arxiv.org/abs/1706.04148?utm_source=chatgpt.com "Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks")).
    

---

## âœ… Summary Table

|Resource|Focus|Why Useful|
|---|---|---|
|OTTO blog/code walkthroughs|Word2Vec + candidate gen & Recall@20 pipeline|Matches exactly multiâ€‘objective sessionâ€‘based competition workflow ([duaibeom.github.io](https://duaibeom.github.io/blog/kaggle_otto?utm_source=chatgpt.com "RecSys Competition - OTTO"), [GitHub](https://github.com/nicolaivicol/otto-recommender?utm_source=chatgpt.com "GitHub - nicolaivicol/otto-recommender: Solution to kaggle competition ..."))|
|GRU4Rec (Hidasi et al.) papers/code|RNN architecture, ranking loss, sessionâ€‘parallel batching|Strong baseline sequential approach ([GitHub](https://github.com/hidasib/GRU4Rec?utm_source=chatgpt.com "GitHub - hidasib/GRU4Rec: GRU4Rec is the original Theano implementation ..."), [arXiv](https://arxiv.org/abs/1511.06939?utm_source=chatgpt.com "Session-based Recommendations with Recurrent Neural Networks"), [hidasi.eu](https://hidasi.eu/assets/pdf/gru4rec_v2_cikm18_slides.pdf?utm_source=chatgpt.com "GRU4Rec v2 Recurrent neural networks with top-k gains for session-based ..."))|
|Sessionâ€‘Recommender Models repo|PyTorch GRU4Rec / SQN / SMORL eval framework|Compare architectures and their ranking metrics ([GitHub](https://github.com/PatrickSVM/Session-Based-Recommender-Models?utm_source=chatgpt.com "Session Based Recommender Models - GitHub"))|
|OTTO recsys dataset repo|Data format, evaluate.py, testset split code|Full readyâ€‘made framework for Recall@K and multiâ€‘task prediction ([GitHub](https://github.com/otto-de/recsys-dataset/blob/main/KAGGLE.md?utm_source=chatgpt.com "recsys-dataset/KAGGLE.md at main Â· otto-de/recsys-dataset"), [GitHub](https://github.com/otto-de/recsys-dataset/blob/main/README.md?utm_source=chatgpt.com "recsys-dataset/README.md at main Â· otto-de/recsys-dataset"))|
|SRâ€‘GNN, GRAINRec, HRNN papers|Advanced architectures (GNN / Hybrid)|Provide nextâ€‘level inspiration or ensemble modules ([arXiv](https://arxiv.org/abs/1811.00855?utm_source=chatgpt.com "Session-based Recommendation with Graph Neural Networks"), [arXiv](https://arxiv.org/abs/2411.09152?utm_source=chatgpt.com "GRAINRec: Graph and Attention Integrated Approach for Real-Time Session-Based Item Recommendations"), [arXiv](https://arxiv.org/abs/1706.04148?utm_source=chatgpt.com "Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks"))|

---

## ðŸ’¡ Suggested Next Steps

1. **Clone and run** the OTTO dataset GitHub repo to understand data format, preâ€‘processing, offline validation, and recall metrics.
    
2. **Run GRU4Rec** baseline (Theano versionâ€”modify if running CPU only) or use a reliable PyTorch/TensorFlow variant; replicate nextâ€‘item prediction & Recall@K results.
    
3. **Implement Word2Vec candidate generator** from session sequences, build simple coâ€‘occurrence rules (â€œclickâ†’clickâ€, â€œclickâ†’orderâ€) as in OTTO solutions.
    
4. **Combine candidates + ranker**: either GRU or LightGBM to score candidates. Evaluate offline with same weighted Recall@K scoring.
    
5. **Explore ensemble or hybrid architectures**: session embeddings + graph-based GNN or attention module added later if time permits.
    

## ðŸ“Š Phase 1: Data Understanding & Exploration (Days 1-3)

### Data Analysis Checklist

```python
# Essential data exploration steps
1. Session Statistics:
   - Session length distribution
   - Products per session (mean, median, 95th percentile)
   - Time gaps between interactions
   - Session duration patterns

2. User Behavior Patterns:
   - Click-to-purchase conversion rates
   - User activity by time of day/week
   - Repeat user vs new user behavior
   - Search query analysis

3. Product Analysis:
   - Product popularity distribution (long tail?)
   - Category hierarchy depth
   - Price distribution and ranges
   - Brand concentration

4. Temporal Patterns:
   - Seasonality effects
   - Weekly/daily cycles
   - Trend changes over time
```

### Data Quality Assessment

- **Missing Value Analysis**: Understand why data is missing (MCAR, MAR, MNAR)
- **Outlier Detection**: Sessions with unusual length, price outliers
- **Data Consistency**: Check for duplicate sessions, impossible timestamps
- **Coverage Analysis**: Product/user coverage in train vs test periods

---

## ðŸ”§ Phase 2: Advanced Feature Engineering (Days 4-8)

### Session-Level Features

```python
# Sequence Features
- session_length, unique_products_viewed
- clicking_velocity (actions per minute)
- time_spent_per_product
- session_bounce_rate
- product_revisit_count
- sequence_entropy (diversity measure)

# Behavioral Features  
- avg_price_viewed, price_variance
- category_diversity_score
- brand_switching_behavior
- search_refinement_count
- cart_abandon_signals
```

### User Historical Features

```python
# User Profile Features
- historical_conversion_rate
- avg_session_length_historical
- favorite_categories (top 3)
- price_sensitivity_score
- brand_loyalty_score
- purchase_frequency
- seasonal_activity_pattern
```

### Product Features

```python
# Static Product Features
- category_popularity_rank
- price_decile_within_category
- brand_market_share
- product_age_days
- review_count, avg_rating

# Dynamic Product Features  
- recent_popularity_trend (7d, 30d)
- co_occurrence_strength
- seasonal_demand_factor
- inventory_availability_proxy
```

### Advanced Interaction Features

```python
# Cross-Features
- user_category_affinity_score
- product_price_vs_user_budget_fit
- time_since_last_purchase_in_category
- product_similarity_to_past_purchases
- complementary_product_signals
```

### Text Features (Search Queries)

```python
# Search Query Analysis
- query_length, word_count
- search_refinement_pattern
- query_category_alignment
- typo_correction_signals
- search_intent_classification
```

---

## ðŸ¤– Phase 3: Model Development Strategy (Days 9-18)

### Tier 1: Baseline Models (Days 9-10)

```python
# Quick Baselines (implement all)
1. Popularity-based: Most clicked/purchased items globally
2. Session-based popularity: Last N interactions â†’ next popular
3. Association Rules: Frequent itemsets for co-occurrence
4. User-based CF: Similar users' preferences
5. Recent trend: Items trending in last 7 days
```

### Tier 2: Embedding-Based Models (Days 11-13)

```python
# Word2Vec-style Approaches
1. Item2Vec:
   - Treat sessions as sentences, products as words
   - Skip-gram or CBOW for product embeddings
   - Session context prediction

2. Session2Vec:
   - Learn session-level representations
   - User behavior pattern embeddings
   - Temporal session embeddings

3. Multi-level Embeddings:
   - Category embeddings
   - Brand embeddings  
   - User segment embeddings
```

### Tier 3: Tree-Based Models (Days 14-16)

```python
# Gradient Boosting Approaches
1. LightGBM Multi-task:
   - Separate models for click/purchase
   - Shared feature engineering pipeline
   - Ranking objective optimization

2. XGBoost with Custom Objective:
   - Custom loss function for Recall@K
   - Feature interaction capture
   - Robust to missing values

3. CatBoost for Categorical Features:
   - Automatic categorical encoding
   - Overfitting resistance
   - Fast inference
```

### Tier 4: Neural Network Models (CPU-Optimized) (Days 17-18)

```python
# Lightweight Deep Learning
1. Shallow Neural Networks:
   - 2-3 hidden layers maximum
   - Batch normalization + dropout
   - Multi-task output (click + purchase)

2. Factorization Machines:
   - Capture feature interactions
   - Linear complexity
   - Great for sparse features

3. Wide & Deep Architecture:
   - Wide: Linear model for memorization
   - Deep: Neural network for generalization
   - Joint training for both tasks
```

---

## ðŸŽ­ Phase 4: Multi-Task Learning Strategy

### Approach 1: Separate Models

```python
# Independent Training
- click_model = LightGBM(objective='rank')
- purchase_model = XGBoost(objective='rank')  
- final_score = w1*click_score + w2*purchase_score
```

### Approach 2: Joint Model

```python
# Shared Architecture
- shared_features â†’ shared_layers â†’ task_specific_heads
- Loss = Î±*click_loss + Î²*purchase_loss
- Hard parameter sharing vs soft parameter sharing
```

### Approach 3: Hierarchical Model

```python
# Sequential Modeling
- Stage 1: Click probability prediction
- Stage 2: Purchase prediction among clicked items
- Feature: predicted_click_probability as input to purchase model
```

---

## ðŸ“ Phase 5: Evaluation & Validation Strategy (Days 19-21)

### Time-Based Cross-Validation

```python
# Temporal Split Strategy
- Training: Weeks 1-3
- Validation: Week 4
- Test: Week 5

# Rolling Window Validation
for i in range(n_folds):
    train_end = base_date + timedelta(weeks=3+i)
    val_start = train_end
    val_end = val_start + timedelta(weeks=1)
```

### Custom Recall@K Implementation

```python
def recall_at_k(y_true, y_pred, k=20):
    """
    Calculate Recall@K for recommendation tasks
    """
    recall_scores = []
    for true_items, pred_items in zip(y_true, y_pred):
        if len(true_items) == 0:
            continue
        
        pred_k = pred_items[:k]
        relevant_retrieved = len(set(true_items) & set(pred_k))
        recall = relevant_retrieved / len(true_items)
        recall_scores.append(recall)
    
    return np.mean(recall_scores)

def joint_recall(click_true, click_pred, purchase_true, purchase_pred, 
                 k=20, click_weight=0.1, purchase_weight=0.9):
    """
    Joint evaluation metric
    """
    click_recall = recall_at_k(click_true, click_pred, k)
    purchase_recall = recall_at_k(purchase_true, purchase_pred, k)
    
    return click_weight * click_recall + purchase_weight * purchase_recall
```

### Validation Strategies

```python
# Group-based validation
- GroupKFold by user_id (prevent data leakage)
- Stratified by session_length (ensure representation)
- Time-series split (realistic scenario)

# Cold-start evaluation
- New users not seen in training
- New products not seen in training
- New user-product combinations
```

---

## ðŸŽ¯ Phase 6: Model Ensemble & Optimization (Days 22-25)

### Ensemble Architecture

```python
# Level 1 Models (Base Models)
models_l1 = {
    'popularity': PopularityModel(),
    'item2vec': Item2VecModel(), 
    'lgb_click': LightGBMClick(),
    'lgb_purchase': LightGBMPurchase(),
    'xgb_multi': XGBoostMultiTask(),
    'nn_multi': NeuralNetMultiTask()
}

# Level 2 Model (Meta-learner)
meta_features = [
    'model_confidence_scores',
    'prediction_diversity',
    'session_complexity_score',
    'user_predictability_score'
]

meta_model = LightGBM(objective='regression')
```

### Ensemble Strategies

```python
# 1. Weighted Average
- Simple weighted combination
- Weights optimized on validation set
- Different weights for click vs purchase

# 2. Rank Fusion
- Combine ranked lists from different models
- Reciprocal Rank Fusion (RRF)
- Borda count method

# 3. Stacking
- Train meta-model on base model predictions
- Cross-validation to generate meta-features
- Prevent overfitting with regularization

# 4. Dynamic Ensemble
- Model selection based on session characteristics
- Route easy sessions to simple models
- Route complex sessions to advanced models
```

### Hyperparameter Optimization

```python
# Bayesian Optimization
import optuna

def objective(trial):
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'num_leaves': trial.suggest_int('num_leaves', 10, 300),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0)
    }
    
    score = cross_validate_model(params)
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

---

## ðŸ›  Phase 7: Implementation & Tools

### Essential Libraries

```python
# Core Data Processing
import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix

# Machine Learning
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import GroupKFold
from sklearn.metrics import ndcg_score

# Embeddings & NLP
from gensim.models import Word2Vec
from sentence_transformers import SentenceTransformer

# Utilities
import joblib
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
```

### Efficient Data Pipeline

```python
# Memory-efficient data loading
def load_data_chunked(filepath, chunk_size=10000):
    chunks = []
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunks.append(preprocess_chunk(chunk))
    return pd.concat(chunks, ignore_index=True)

# Feature caching
def cache_features(func):
    def wrapper(*args, **kwargs):
        cache_key = f"{func.__name__}_{hash(str(args))}"
        if os.path.exists(f"cache/{cache_key}.pkl"):
            return joblib.load(f"cache/{cache_key}.pkl")
        result = func(*args, **kwargs)
        joblib.dump(result, f"cache/{cache_key}.pkl")
        return result
    return wrapper
```

---

## ðŸ“… Phase 8: Competition Endgame (Days 26-30)

### Final Model Selection

```python
# Model selection criteria
1. Cross-validation score stability
2. Private leaderboard correlation
3. Model inference speed
4. Prediction diversity
5. Robustness to data drift
```

### Submission Strategy

```python
# Multiple submissions
- Conservative: Best single model
- Aggressive: Complex ensemble
- Balanced: Simple ensemble of top 3 models

# Submission file format
submission = pd.DataFrame({
    'session_id': test_sessions,
    'click_predictions': click_pred_lists,  # List of product IDs
    'purchase_predictions': purchase_pred_lists
})
```

### Risk Management

```python
# Overfitting prevention
- Hold-out validation set untouched until final selection
- Monitor public leaderboard shake-up patterns
- Simple models as safety net

# Code robustness
- Extensive error handling
- Data validation checks
- Reproducible random seeds
```

---

## ðŸ† Advanced Competition Tactics

### Data Augmentation

```python
# Session augmentation techniques  
1. Session truncation: Use partial sessions for training
2. Session extension: Add synthetic interactions
3. Temporal shifting: Train on different time windows
4. User simulation: Generate realistic user journeys
```

### Feature Selection

```python
# Recursive feature elimination
from sklearn.feature_selection import RFE

# Correlation analysis
def remove_correlated_features(df, threshold=0.95):
    corr_matrix = df.corr().abs()
    upper_tri = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    return [column for column in upper_tri.columns 
            if any(upper_tri[column] > threshold)]

# Feature importance analysis
def analyze_feature_importance(model, feature_names):
    importance = model.feature_importances_
    return pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)
```

### Cold Start Handling

```python
# New user strategies
1. Demographic-based recommendations
2. Popular items in user's first session category
3. Location-based preferences
4. Time-based trending items

# New product strategies  
1. Content-based similarity to existing products
2. Category-based recommendations
3. Brand-based associations
4. Price-range popularity
```

---

## ðŸ“š Learning Resources & References

### Essential Papers

1. **"Session-based Recommendations with Recurrent Neural Networks"** (GRU4Rec)
2. **"Neural Attentive Session-based Recommendation"** (NARM)
3. **"Self-Attentive Sequential Recommendation"** (SASRec)
4. **"Session-based Recommendation with Graph Neural Networks"** (SR-GNN)

### Kaggle Competition Studies

- **OTTO Recommender System**: Most similar competition
- **H&M Personalized Fashion Recommendations**: Multi-task learning
- **Instacart Market Basket Analysis**: Purchase prediction

### Winning Solution Analysis

```python
# Study these GitHub repositories:
1. OTTO competition winners
2. RecSys challenge solutions  
3. Session-based recommendation benchmarks
4. Multi-task learning implementations
```

---

## âš¡ Quick Win Strategies

### Week 1 Priorities

1. âœ… Implement popularity baseline (2 hours)
2. âœ… Build validation framework (4 hours)
3. âœ… Create feature engineering pipeline (8 hours)
4. âœ… Item2Vec embeddings (6 hours)

### Week 2 Priorities

1. âœ… LightGBM multi-task model (8 hours)
2. âœ… Advanced feature engineering (12 hours)
3. âœ… Cross-validation optimization (4 hours)
4. âœ… Ensemble framework (6 hours)

### Week 3 Priorities

1. âœ… Model refinement and tuning (12 hours)
2. âœ… Ensemble optimization (8 hours)
3. âœ… Final validation and testing (6 hours)
4. âœ… Submission preparation (4 hours)

---

## ðŸš¨ Common Pitfalls to Avoid

### Data Leakage

- âŒ Using future information in features
- âŒ User information from test period in training
- âŒ Global statistics including test data

### Overfitting Indicators

- âŒ Large gap between CV and public LB
- âŒ Performance degradation with more features
- âŒ Ensemble performing worse than single models

### Validation Mistakes

- âŒ Random splits instead of time-based
- âŒ Not respecting user boundaries
- âŒ Ignoring cold-start scenarios

---

## ðŸŽ¯ Success Metrics & KPIs

### Technical Metrics

- **Recall@20 > 0.15** (competitive threshold)
- **Joint Recall Score > 0.12** (winning range)
- **Model Inference < 100ms per session**

### Process Metrics

- **Feature Engineering Iterations**: Minimum 3 rounds
- **Model Experiments**: 15+ different approaches
- **Ensemble Combinations**: 5+ different strategies

---

This comprehensive roadmap provides a systematic approach to winning the hackathon while being realistic about resource constraints. The key is to build incrementally, validate continuously, and focus on what works rather than what's theoretically optimal.
